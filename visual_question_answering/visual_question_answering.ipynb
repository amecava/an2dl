{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('3.8.6')",
   "metadata": {
    "interpreter": {
     "hash": "2121d40ec6e6dcc78216aaece13758c0bf2c06d590f255cf654b7388266a3141"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Input Data\n",
    "# -----------------\n",
    "# Parameters:\n",
    "\n",
    "# Upload Input Data (True, False)\n",
    "UPLOAD_DATA = False\n",
    "\n",
    "# -----------------\n",
    "\n",
    "if UPLOAD_DATA:\n",
    "    from google.colab import files\n",
    "    files.upload()\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules Import\n",
    "# --------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import math\n",
    "import glob\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, add, Conv2D, Reshape\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing import image, text, sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "%matplotlib inline\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Available Models\n",
    "# -----------------------------------\n",
    "\n",
    "class MODEL(enum.Enum):\n",
    "    VGG19 = \"VGG19\"\n",
    "    AUTOENCODER = \"Autoencoder\"\n",
    "\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Parameters\n",
    "# -----------------\n",
    "# Parameters:\n",
    "\n",
    "# Image Height\n",
    "IMG_H = 400\n",
    "\n",
    "# Image Width\n",
    "IMG_W = 700\n",
    "\n",
    "# Text Data Augmentation (True, False)\n",
    "TEXT_AUGMENTATION = False\n",
    "\n",
    "# Feature Extraction Model\n",
    "FEATURE_EXTRACTION = MODEL.AUTOENCODER.value\n",
    "\n",
    "# Train Autoencoder Model (True, False)\n",
    "TRAIN_AUTOENCODER = False\n",
    "\n",
    "# Train Attention Model (True, False)\n",
    "TRAIN_ATTENTION = True\n",
    "\n",
    "# Test Prediction (True, False)\n",
    "TEST_PREDICTION = False\n",
    "\n",
    "# -----------------\n",
    "\n",
    "labels_dict = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '5': 5,\n",
    "    'apple': 6,\n",
    "    'baseball': 7,\n",
    "    'bench': 8,\n",
    "    'bike': 9,\n",
    "    'bird': 10,\n",
    "    'black': 11,\n",
    "    'blanket': 12,\n",
    "    'blue': 13,\n",
    "    'bone': 14,\n",
    "    'book': 15,\n",
    "    'boy': 16,\n",
    "    'brown': 17,\n",
    "    'cat': 18,\n",
    "    'chair': 19,\n",
    "    'couch': 20,\n",
    "    'dog': 21,\n",
    "    'floor': 22,\n",
    "    'food': 23,\n",
    "    'football': 24,\n",
    "    'girl': 25,\n",
    "    'grass': 26,\n",
    "    'gray': 27,\n",
    "    'green': 28,\n",
    "    'left': 29,\n",
    "    'log': 30,\n",
    "    'man': 31,\n",
    "    'monkey bars': 32,\n",
    "    'no': 33,\n",
    "    'nothing': 34,\n",
    "    'orange': 35,\n",
    "    'pie': 36,\n",
    "    'plant': 37,\n",
    "    'playing': 38,\n",
    "    'red': 39,\n",
    "    'right': 40,\n",
    "    'rug': 41,\n",
    "    'sandbox': 42,\n",
    "    'sitting': 43,\n",
    "    'sleeping': 44,\n",
    "    'soccer': 45,\n",
    "    'squirrel': 46,\n",
    "    'standing': 47,\n",
    "    'stool': 48,\n",
    "    'sunny': 49,\n",
    "    'table': 50,\n",
    "    'tree': 51,\n",
    "    'watermelon': 52,\n",
    "    'white': 53,\n",
    "    'wine': 54,\n",
    "    'woman': 55,\n",
    "    'yellow': 56,\n",
    "    'yes': 57\n",
    "}\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "# -----------------\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Directories\n",
    "# ------------------\n",
    "# Parameters:\n",
    "\n",
    "# Root Directory\n",
    "ROOT_DIR = pathlib.Path(\".\")\n",
    "\n",
    "# ------------------\n",
    "\n",
    "annotations_file = ROOT_DIR / \"train_questions_annotations.json\"\n",
    "augmented_file = ROOT_DIR / \"train_augmented_questions_annotations.json\"\n",
    "\n",
    "if TEXT_AUGMENTATION:\n",
    "    processed_file = ROOT_DIR / \"processed_train_augmented_questions_annotations.json\"\n",
    "else:\n",
    "    processed_file = = ROOT_DIR / \"processed_train_questions_annotations.json\"\n",
    "\n",
    "temp_directory = ROOT_DIR / \"temps\"\n",
    "\n",
    "images_directory = ROOT_DIR / \"images\"\n",
    "features_directory = ROOT_DIR / 'features'\n",
    "\n",
    "autoencoder_checkpoint_directory = temp_directory\n",
    "autoencoder_checkpoint = autoencoder_checkpoint_directory / \"autoencoder_checkpoint\"\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed\n",
    "# -----------\n",
    "# Parameters:\n",
    "\n",
    "# Random Seed\n",
    "SEED = 1000\n",
    "\n",
    "# -----------\n",
    "\n",
    "tf.random.set_seed(SEED) \n",
    "\n",
    "# -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Augmentation\n",
    "# -----------------\n",
    "# Parameters:\n",
    "\n",
    "# Force Reload Text Augmentation (True, False)\n",
    "FORCE_RELOAD = False\n",
    "\n",
    "# Augmentation Frequency\n",
    "FREQUENCY = 5000\n",
    "\n",
    "# -----------------\n",
    "\n",
    "if TEXT_AUGMENTATION and (FORCE_RELOAD or not os.path.exists(augmented_file)):\n",
    "    !pip install textattack\n",
    "\n",
    "    from textattack.augmentation import EmbeddingAugmenter\n",
    "    aug = EmbeddingAugmenter()\n",
    "\n",
    "    augmented_data = []\n",
    "    class_frequency = {}\n",
    "\n",
    "    for i in range(58):\n",
    "        class_frequency[i] = 0\n",
    "        \n",
    "    for i in tqdm(annotations_file):\n",
    "        value = anno[i]\n",
    "        answer = value['answer']\n",
    "        class_frequency[labels_dict[answer]] += 1\n",
    "        \n",
    "    for i in tqdm(annotations_file):\n",
    "        value = annotations_file[i]\n",
    "        image_path = value['image_id']\n",
    "        question = value['question']\n",
    "        answer = value['answer']\n",
    "        \n",
    "        weight = int(FREQUENCY / class_frequency[labels_dict[answer]])\n",
    "        \n",
    "        if weight <= 1:\n",
    "            augmented_data.append({\n",
    "                'image_id': str(image_path), \n",
    "                'question': question, \n",
    "                'answer': answer})\n",
    "        else:\n",
    "            for j in range(weight):\n",
    "                augmented_question = aug.augment(question)\n",
    "                augmented_data.append({\n",
    "                    'image_id': str(image_path), \n",
    "                    'question': augmented_question[0], \n",
    "                    'answer': answer})\n",
    "\n",
    "    json.dump(augmented_data, open(augmented_file, 'w'))\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations Processing\n",
    "# ----------------------\n",
    "\n",
    "def process_question_annotation(annotations):\n",
    "    anno = json.load(open(annotations, 'r'))\n",
    "\n",
    "    data = []\n",
    "    for value in tqdm(anno):\n",
    "        image_path = images_directory / (value['image_id']+'.png') \n",
    "        question = value['question'][0]\n",
    "        answer =  value['answer']\n",
    "        data.append({\n",
    "            'img_path': str(image_path), \n",
    "            'question': question, \n",
    "            'answer': answer})\n",
    "    \n",
    "    json.dump(data, open(processed_file, 'w'))\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "if TEXT_AUGMENTATION:\n",
    "    process_question_annotation(augmented_file)\n",
    "else:\n",
    "    process_question_annotation(annotations_file)\n",
    "\n",
    "train_data = processed_file\n",
    "\n",
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers Statistics\n",
    "# ------------------\n",
    "\n",
    "anno = json.load(open(processed_file, 'r'))\n",
    "\n",
    "new_data = []\n",
    "class_frequency = {}\n",
    "\n",
    "for i in range(58):\n",
    "    class_frequency[i] = 0\n",
    "    \n",
    "for value in tqdm(anno):\n",
    "    answer =  value['answer']\n",
    "    class_frequency[labels_dict[answer]] += 1\n",
    "    \n",
    "print(class_frequency)\n",
    "\n",
    "answer_freq= defaultdict(int)\n",
    "\n",
    "for answer in list(map(itemgetter('answer'), train_data)):\n",
    "    answer_freq[answer] += 1\n",
    "\n",
    "max_answers = len(answer_freq)\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Data Reduction\n",
    "# ---------------------\n",
    "\n",
    "def select_top_answers_data(questions_list, answer_list, images_list, k):\n",
    "\tanswer_freq= defaultdict(int)\n",
    "\n",
    "\tfor answer in answer_list:\n",
    "\t\tanswer_freq[answer] += 1\n",
    "\n",
    "\tsorted_freq = sorted(answer_freq.items(), key=operator.itemgetter(1), reverse=True)[0: k]\n",
    "\ttop_answers, top_freq = zip(*sorted_freq)\n",
    " \n",
    "\tnew_questions_list=[]\n",
    "\tnew_answer_list=[]\n",
    "\tnew_images_list=[]\n",
    "\n",
    "\tfor question, answer, image in zip(questions_list, answer_list, images_list):\n",
    "\t\tif answer in top_answers:\n",
    "\t\t\tnew_questions_list.append(question)\n",
    "\t\t\tnew_answer_list.append(answer)\n",
    "\t\t\tnew_images_list.append(image)\n",
    "\n",
    "\treturn (new_questions_list, new_answer_list, new_images_list, top_answers)\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "questions_train, answer_train, images_train, top_answers = select_top_answers_data(\n",
    "    list(map(itemgetter('question'), train_data)), \n",
    "    list(map(itemgetter('answer'), train_data)), \n",
    "    list(map(itemgetter('img_path'), train_data)), \n",
    "    max_answers)\n",
    "\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Dimensions\n",
    "# -----------------------------\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.VGG.value:\n",
    "    IMG_W /= 2 \n",
    "    IMG_H /= 2\n",
    "else:\n",
    "    IMG_W = 648\n",
    "    IMG_H = 324\n",
    "\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Splitting\n",
    "# -------------------\n",
    "# Parameters:\n",
    "\n",
    "# Dataframe Split\n",
    "DF_SPLIT = 0.8\n",
    "\n",
    "# -------------------\n",
    "\n",
    "train_dir = images_directory\n",
    "\n",
    "image_filenames = next(os.walk(train_dir))[2]\n",
    "\n",
    "data = []\n",
    "for row_index, image_name in tqdm(enumerate(image_filenames)):\n",
    "    data.append(image_name)\n",
    "\n",
    "DATAFRAME = pd.DataFrame(data, columns=['filename'])\n",
    "\n",
    "probs = np.random.rand(len(DATAFRAME))\n",
    "training_mask = probs < DF_SPLIT\n",
    "validation_mask = (probs >= DF_SPLIT)\n",
    "\n",
    "DF_TRAIN = DATAFRAME[training_mask]\n",
    "DF_VAL = DATAFRAME[validation_mask]\n",
    "\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Data Generator\n",
    "# --------------------------\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.AUTOENCODER.value:\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    CLASS_MODE = \"input\"\n",
    "    CLASSES = None\n",
    "\n",
    "    # Training Data Generator\n",
    "    train_gen = train_datagen.flow_from_dataframe(\n",
    "        DF_TRAIN,\n",
    "        train_dir,\n",
    "        batch_size=AUTOENCODER_BATCH_SIZE,\n",
    "        target_size=(IMG_H, IMG_W),\n",
    "        class_mode=CLASS_MODE,\n",
    "        classes=CLASSES,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Validation Data Generator\n",
    "    val_gen = valid_datagen.flow_from_dataframe(\n",
    "        DF_VAL,\n",
    "        train_dir,\n",
    "        batch_size=AUTOENCODER_BATCH_SIZE,\n",
    "        target_size=(IMG_H, IMG_W),\n",
    "        class_mode=CLASS_MODE,\n",
    "        classes=CLASSES,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Model Architecture\n",
    "# ------------------------------\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.AUTOENCODER.value:\n",
    "    input_shape = [IMG_H, IMG_W, 3]\n",
    "\n",
    "    # Encoder\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(tf.keras.layers.Input(input_shape))\n",
    "\n",
    "    encoder.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    encoder.add(tf.keras.layers.ReLU())\n",
    "    encoder.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "    encoder.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    encoder.add(tf.keras.layers.ReLU())\n",
    "    encoder.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "    encoder.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    encoder.add(tf.keras.layers.ReLU())\n",
    "    encoder.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "    encoder.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    encoder.add(tf.keras.layers.ReLU())\n",
    "    encoder.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "    encoder.add(tf.keras.layers.Flatten())\n",
    "    encoder.add(Dense(1024,activation='relu'))\n",
    "\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(tf.keras.layers.Input([1024]))\n",
    "    decoder.add(Dense(2048,activation='relu'))\n",
    "    decoder.add(tf.keras.layers.Reshape((4, 8, 64)))\n",
    "\n",
    "    decoder.add(tf.keras.layers.UpSampling2D(size=(3, 3), interpolation=\"bilinear\"))\n",
    "    decoder.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    decoder.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    decoder.add(tf.keras.layers.UpSampling2D(size=(3, 3), interpolation=\"bilinear\"))\n",
    "    decoder.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    decoder.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    decoder.add(tf.keras.layers.UpSampling2D(size=(3, 3), interpolation=\"bilinear\"))\n",
    "    decoder.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    decoder.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    decoder.add(tf.keras.layers.UpSampling2D(size=(3, 3), interpolation=\"bilinear\"))\n",
    "    decoder.add(tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=tf.keras.activations.sigmoid))\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = tf.keras.Sequential()\n",
    "    autoencoder.add(tf.keras.layers.Input(input_shape))\n",
    "    autoencoder.add(encoder)\n",
    "    autoencoder.add(decoder)\n",
    "\n",
    "\n",
    "    autoencoder.summary()\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Model Optimization\n",
    "# ------------------------------\n",
    "# Parameters:\n",
    "\n",
    "# Early Stopping (True, False)\n",
    "EARLY_STOP = True\n",
    "\n",
    "# Learning Rate \n",
    "LR = 1e-4\n",
    "\n",
    "# Validation Metrics\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "# ------------------------------\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.AUTOENCODER.value:\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mse\", metrics=METRICS)\n",
    "\n",
    "    callbacks = []\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(autoencoder_checkpoint_directory, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto',save_freq='epoch',period=3)\n",
    "    callbacks.append(cp_callback)\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Fitting\n",
    "# -------------------\n",
    "# Parameters:\n",
    "\n",
    "# Number of Epochs\n",
    "AE_EPOCHS = 200\n",
    "\n",
    "# -------------------\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.AUTOENCODER.value:\n",
    "    if TRAIN_AUTOENCODER:\n",
    "        autoencoder.fit_generator(train_gen,\n",
    "            epochs=AUTOENCODER_EPOCHS,\n",
    "            validation_data=val_gen,\n",
    "            callbacks=callbacks)\n",
    "    else:\n",
    "        autoencoder = tf.keras.models.load_model(autoencoder_checkpoint)\n",
    "        decoder = autoencoder._layers.pop()\n",
    "        encoder = autoencoder._layers.pop()\n",
    "\n",
    "# -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Summary\n",
    "# ---------------\n",
    "\n",
    "encoder.summary()\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function\n",
    "# ---------------------------\n",
    "\n",
    "def image_feature_extractor(target_path, image_list, batch_size, model, model_type):\n",
    "\tprogbar = utils.Progbar(int(np.ceil(len(image_list) / float(batch_size))))\n",
    "    \n",
    "\tfor (b, i) in enumerate(range(0, len(image_list), batch_size)):\n",
    "\t\tprogbar.update(b + 1)\n",
    "\t\t\n",
    "\t\tbatch_range = range(i, min(i + BATCH_SIZE, len(image_list)))\n",
    "\t\tbatchPaths = image_list[batch_range[0]: batch_range[-1]+1]\n",
    "\n",
    "\t\tbatchImages = []\n",
    "\t\tbatchIds = []\n",
    "\n",
    "\t\tfor imagePath in batchPaths:\n",
    "\t\t\timg = image.load_img(str(ROOT_DIR / 'images' / imagePath), target_size=(IMG_H, IMG_W))\n",
    "\t\t\timg = image.img_to_array(img)\n",
    "    \n",
    "\t\t\timg = np.expand_dims(img, axis=0)\n",
    "\t\t\timg = preprocess_input(img)\n",
    "    \n",
    "\t\t\tbatchImages.append(img)\n",
    "\t\t\tbatchIds.append(imagePath.split('.')[0][-6:])\n",
    "        \n",
    "\t\tbatchImages = np.vstack(batchImages)\n",
    "\n",
    "\t\tfeatures = model.predict(batchImages)\n",
    "\n",
    "        if model_type == MODEL.AUTOENCODER.value:\n",
    "\t\t    features = np.expand_dims(features, axis=1)\n",
    "        else:\n",
    "            features = tf.reshape(features, (features.shape[0],-1, features.shape[3]))\n",
    "\n",
    "\t\tfor id, feat in zip(batchIds, features):\n",
    "\t\t\tnp.save(os.path.join(target_path, id), feat)\n",
    "\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "# -----------------\n",
    "# Parameters:\n",
    "\n",
    "# Force Reload Feature Extraction (True, False)\n",
    "FORCE_RELOAD = False\n",
    "\n",
    "# Batch Size\n",
    "FE_BATCH = 100\n",
    "\n",
    "# -----------------\n",
    "\n",
    "if FEATURE_EXTRACTION == MODEL.AUTOENCODER.value:\n",
    "    FE_MODEL = encoder\n",
    "else:\n",
    "    FE_MODEL = VGG19(weights=\"imagenet\", include_top=False,  input_tensor=Input(shape=(IMG_H, IMG_W, 3)))\n",
    "\n",
    "# -----------------\n",
    "\n",
    "if FORCE_RELOAD or not os.path.exists(features_directory)):\n",
    "\n",
    "    image_list = os.listdir(images_directory)\n",
    "\n",
    "    if not os.path.exists(features_directory):\n",
    "        os.mkdir(features_directory)\n",
    "    \n",
    "    image_feature_extractor(features_directory, image_list, FE_BATCH, FE_MODEL)\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing Function\n",
    "# ------------------------\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "    commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "    punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                    '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                    '>', '<', '@', '`', ',', '?', '!']\n",
    "    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n",
    "                    \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n",
    "                    \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n",
    "                    \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n",
    "                    \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n",
    "                    \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n",
    "                    \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n",
    "                    \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n",
    "                    \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n",
    "                    \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n",
    "                    \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n",
    "                    \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n",
    "                    \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n",
    "                    \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n",
    "                    \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n",
    "                    \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n",
    "                    \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n",
    "                    \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n",
    "                    \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n",
    "                    \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n",
    "                    \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n",
    "                    \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n",
    "\n",
    "    inText = sentence.replace('\\n', ' ')\n",
    "    inText = inText.replace('\\t', ' ')\n",
    "    inText = inText.strip()\n",
    "    outText = inText\n",
    "    for p in punct:\n",
    "        if (p + ' ' in inText or ' ' + p in inText) or \\\n",
    "           (re.search(commaStrip, inText) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "    outText = periodStrip.sub(\"\", outText, re.UNICODE)\n",
    "    outText = outText.lower().split()\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText\n",
    "\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing\n",
    "# ---------------\n",
    "\n",
    "questions_train_processed = pd.Series(questions_train).apply(process_sentence)\n",
    "\n",
    "tok = text.Tokenizer(filters='')\n",
    "tok.fit_on_texts(questions_train_processed)\n",
    "\n",
    "question_data_train = tok.texts_to_sequences(questions_train_processed)\n",
    "\n",
    "question_len = [len(text) for text in question_data_train]\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.distplot(question_len, color='red')\n",
    "plt.title('Distribution of Question length')\n",
    "plt.xlabel('Length of Question')\n",
    "plt.ylabel('Question count')\n",
    "plt.xlim(0, 30)\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, 11):\n",
    "    print(10 * i, 'percentile value is', np.percentile(question_len, 10*i))\n",
    "\n",
    "for i in range(0, 11):\n",
    "    print(90 + i, 'percentile value is',np.percentile(question_len, 90+i))\n",
    "\n",
    "MAX_LEN = 21\n",
    "\n",
    "question_data_train=sequence.pad_sequences(question_data_train, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers Processing\n",
    "# ------------------\n",
    "\n",
    "def get_answers_matrix(answers, encoder):\n",
    "\ty = encoder.transform(answers)\n",
    "\tnb_classes = encoder.classes_.shape[0]\n",
    "\tY = utils.to_categorical(y, nb_classes)\n",
    "\treturn Y\n",
    "\n",
    "# ------------------\n",
    "\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "labelencoder.fit(answer_train)\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations Splitting\n",
    "# ---------------------\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size= 0.25,random_state=42)\n",
    "\n",
    "for train_index, val_index in sss.split(images_train, answer_train):\n",
    "    TRAIN_INDEX = train_index\n",
    "    VAL_INDEX = val_index\n",
    "\n",
    "image_list_tr, image_list_vl = np.array(images_train)[TRAIN_INDEX.astype(int)], np.array(images_train)[VAL_INDEX.astype(int)]\n",
    "\n",
    "question_tr, question_vl = question_data_train[TRAIN_INDEX], question_data_train[VAL_INDEX]\n",
    "\n",
    "answer_matrix = get_answers_matrix(answer_train, labelencoder)\n",
    "answer_tr, answer_vl = answer_matrix[TRAIN_INDEX], answer_matrix[VAL_INDEX]\n",
    "\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Maps\n",
    "# --------------\n",
    "\n",
    "class AttentionMaps(tf.keras.layers.Layer):\n",
    "  def __init__(self, dim_k, reg_value, **kwargs):\n",
    "    super(AttentionMaps, self).__init__(**kwargs)\n",
    "\n",
    "    self.dim_k = dim_k\n",
    "    self.reg_value = reg_value\n",
    "\n",
    "    self.Wv = Dense(self.dim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))\n",
    "    self.Wq = Dense(self.dim_k, activation=None,\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))\n",
    "\n",
    "  def call(self, image_feat, ques_feat):\n",
    "    # Affinity Matrix C\n",
    "    # (QT)(Wb)V \n",
    "    C = tf.matmul(ques_feat, tf.transpose(image_feat, perm=[0,2,1])) # [b, 23, 49]\n",
    "    # tanh((QT)(Wb)V)\n",
    "    C = tf.keras.activations.tanh(C) \n",
    "\n",
    "    # (Wv)V\n",
    "    WvV = self.Wv(image_feat)                             # [b, 49, dim_k]\n",
    "    # (Wq)Q\n",
    "    WqQ = self.Wq(ques_feat)                              # [b, 23, dim_k]\n",
    "\n",
    "    # ((Wq)Q)C\n",
    "    WqQ_C = tf.matmul(tf.transpose(WqQ, perm=[0,2,1]), C) # [b, k, 49]\n",
    "    WqQ_C = tf.transpose(WqQ_C, perm =[0,2,1])            # [b, 49, k]\n",
    "\n",
    "    # ((Wv)V)CT                                           # [b, k, 23]\n",
    "    WvV_C = tf.matmul(tf.transpose(WvV, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1]))  \n",
    "                        \n",
    "    WvV_C = tf.transpose(WvV_C, perm =[0,2,1])            # [b, 23, k]\n",
    "\n",
    "    #---------------image attention map------------------\n",
    "    # We find \"Hv = tanh((Wv)V + ((Wq)Q)C)\" ; H_v shape [49, k]\n",
    "\n",
    "    H_v = WvV + WqQ_C                                     # (Wv)V + ((Wq)Q)C\n",
    "    H_v = tf.keras.activations.tanh(H_v)                  # tanh((Wv)V + ((Wq)Q)C) \n",
    "\n",
    "    #---------------question attention map---------------\n",
    "    # We find \"Hq = tanh((Wq)Q + ((Wv)V)CT)\" ; H_q shape [23, k]\n",
    "\n",
    "    H_q = WqQ + WvV_C                                     # (Wq)Q + ((Wv)V)CT\n",
    "    H_q = tf.keras.activations.tanh(H_q)                  # tanh((Wq)Q + ((Wv)V)CT) \n",
    "        \n",
    "    return [H_v, H_q]                                     # [b, 49, k], [b, 23, k]\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'dim_k': self.dim_k,\n",
    "        'reg_value': self.reg_value\n",
    "    }\n",
    "    base_config = super(AttentionMaps, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# --------------\n",
    "\n",
    "layer = AttentionMaps(64, 0.001)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = AttentionMaps.from_config(config)\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Vector\n",
    "# --------------\n",
    "\n",
    "class ContextVector(tf.keras.layers.Layer):\n",
    "  def __init__(self, reg_value, **kwargs):\n",
    "    super(ContextVector, self).__init__(**kwargs)\n",
    "\n",
    "    self.reg_value = reg_value\n",
    "\n",
    "    self.w_hv = Dense(1, activation='softmax',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))\n",
    "    self.w_hq = Dense(1, activation='softmax',\\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) \n",
    "    \n",
    "\n",
    "  def call(self, image_feat, ques_feat, H_v, H_q):\n",
    "    # attention probabilities of each image region vn; a_v = softmax(wT_hv * H_v)\n",
    "    a_v = self.w_hv(H_v)                               # [b, 49, 1]\n",
    "\n",
    "    # attention probabilities of each word qt ;        a_q = softmax(wT_hq * H_q)\n",
    "    a_q = self.w_hq(H_q)                               # [b, 23, 1]\n",
    "\n",
    "    # context vector for image\n",
    "    v = a_v * image_feat                               # [b, 49, dim_d]\n",
    "    v = tf.reduce_sum(v, 1)                            # [b, dim_d]\n",
    "\n",
    "    # context vector for question\n",
    "    q = a_q * ques_feat                                # [b, 23, dim_d]\n",
    "    q = tf.reduce_sum(q, 1)                            # [b, dim_d]\n",
    "\n",
    "\n",
    "    return [v, q]\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'reg_value': self.reg_value\n",
    "    }\n",
    "    base_config = super(ContextVector, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# --------------\n",
    "\n",
    "layer = ContextVector(0.001)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = ContextVector.from_config(config)\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase Level Features\n",
    "# ---------------------\n",
    "\n",
    "class PhraseLevelFeatures(tf.keras.layers.Layer):\n",
    "  def __init__(self, dim_d, **kwargs):\n",
    "    super(PhraseLevelFeatures, self).__init__(**kwargs)\n",
    "    \n",
    "    self.dim_d = dim_d\n",
    "    \n",
    "    self.conv_unigram = Conv1D(self.dim_d, kernel_size=1, strides=1,\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) \n",
    "    self.conv_bigram =  Conv1D(self.dim_d, kernel_size=2, strides=1, padding='same',\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) \n",
    "    self.conv_trigram = Conv1D(self.dim_d, kernel_size=3, strides=1, padding='same',\\\n",
    "                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) \n",
    "\n",
    "\n",
    "  def call(self, word_feat):\n",
    "    # phrase level unigram features\n",
    "    x_uni = self.conv_unigram(word_feat)                    # [b, 23, dim_d]\n",
    "\n",
    "    # phrase level bigram features\n",
    "    x_bi  = self.conv_bigram(word_feat)                     # [b, 23, dim_d]\n",
    "\n",
    "    # phrase level trigram features\n",
    "    x_tri = self.conv_trigram(word_feat)                    # [b, 23, dim_d]\n",
    "\n",
    "    # Concat\n",
    "    x = tf.concat([tf.expand_dims(x_uni, -1),\\\n",
    "                    tf.expand_dims(x_bi, -1),\\\n",
    "                    tf.expand_dims(x_tri, -1)], -1)         # [b, 23, dim_d, 3]\n",
    "\n",
    "    # https://stackoverflow.com/a/36853403\n",
    "    # Max-pool across n-gram features; over-all phrase level feature\n",
    "    x = tf.reduce_max(x, -1)                                # [b, 23, dim_d]\n",
    "\n",
    "    return x\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'dim_d': self.dim_d\n",
    "    }\n",
    "    base_config = super(PhraseLevelFeatures, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "layer = PhraseLevelFeatures(32)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = PhraseLevelFeatures.from_config(config)\n",
    "\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Function\n",
    "# ---------------------------\n",
    "\n",
    "def build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value):\n",
    "    # inputs \n",
    "    image_input = Input(shape=(1, 1024, ), name='Image_Input')\n",
    "    ques_input = Input(shape=(MAX_LEN, ), name='Question_Input')\n",
    "\n",
    "    # image feature; (Wb)V                                          # [b, 49, dim_d]\n",
    "    image_feat = Dense(dim_d, activation=None, name='Image_Feat_Dense',\\\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(image_input)\n",
    "    image_feat = Dropout(d_rate, seed=1)(image_feat)\n",
    "\n",
    "    # word level\n",
    "    ques_feat_w = Embedding(input_dim=vocab_size, output_dim=dim_d, input_length=max_seq_len,\\\n",
    "                            mask_zero=True)(ques_input)\n",
    "    \n",
    "    Hv_w, Hq_w = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Word')(image_feat, ques_feat_w)\n",
    "    v_w, q_w = ContextVector(reg_value, name='ContextVector_Word')(image_feat, ques_feat_w, Hv_w, Hq_w)\n",
    "    feat_w = tf.add(v_w,q_w)\n",
    "    h_w = Dense(dim_d, activation='tanh', name='h_w_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(feat_w)\n",
    "\n",
    "    # phrase level\n",
    "    ques_feat_p = PhraseLevelFeatures(dim_d, name='PhraseLevelFeatures')(ques_feat_w)\n",
    "\n",
    "    Hv_p, Hq_p = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Phrase')(image_feat, ques_feat_p)\n",
    "    v_p, q_p = ContextVector(reg_value, name='ContextVector_Phrase')(image_feat, ques_feat_p, Hv_p, Hq_p)\n",
    "    feat_p = concatenate([tf.add(v_p,q_p), h_w], -1) \n",
    "    h_p = Dense(dim_d, activation='tanh', name='h_p_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(feat_p)\n",
    "\n",
    "    # sentence level\n",
    "    ques_feat_s = LSTM(dim_d, return_sequences=True, input_shape=(None, max_seq_len, dim_d),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(ques_feat_p)\n",
    "\n",
    "    Hv_s, Hq_s = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Sent')(image_feat, ques_feat_s)\n",
    "    v_s, q_s = ContextVector(reg_value, name='ContextVector_Sent')(image_feat, ques_feat_p, Hv_s, Hq_s)\n",
    "    feat_s = concatenate([tf.add(v_s,q_s), h_p], -1) \n",
    "    h_s = Dense(2*dim_d, activation='tanh', name='h_s_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(feat_s)\n",
    "\n",
    "    z   = Dense(2*dim_d, activation='tanh', name='z_Dense',\\\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\\\n",
    "                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(h_s)\n",
    "    z   = Dropout(d_rate, seed=16)(z)\n",
    "\n",
    "    # result\n",
    "    result = Dense(max_answers, activation='softmax')(z)\n",
    "\n",
    "    model = Model(inputs=[image_input, ques_input], outputs=result)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Creation\n",
    "# ----------------\n",
    "# Parameters:\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 300\n",
    "\n",
    "# Buffer Size\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "# ----------------\n",
    "\n",
    "def map_func(img_name, ques, ans):\n",
    "    img_path = img_name.decode(\"utf-8\")\n",
    "    img_path = img_path.replace('images', 'features')\n",
    "    img_path = img_path.replace('png', 'npy')\n",
    "    img_tensor = np.load(img_path)\n",
    "    return img_tensor, ques, ans\n",
    "\n",
    "# ----------------\n",
    "\n",
    "dataset_tr = tf.data.Dataset.from_tensor_slices((image_list_tr, question_tr, answer_tr))\n",
    "\n",
    "dataset_tr = dataset_tr.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_tr = dataset_tr.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_vl = tf.data.Dataset.from_tensor_slices((image_list_vl, question_vl, answer_vl))\n",
    "\n",
    "dataset_vl = dataset_vl.map(lambda item1, item2, item3: tf.numpy_function(\n",
    "    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_vl = dataset_vl.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_vl = dataset_vl.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "# ------------------\n",
    "# Parameters:\n",
    "\n",
    "EPOCHS      = 50\n",
    "\n",
    "max_answers = max_answers\n",
    "max_seq_len = MAX_LEN\n",
    "vocab_size  = len(tok.word_index) + 1\n",
    "\n",
    "dim_d       = 512\n",
    "dim_k       = 256\n",
    "l_rate      = 1e-4\n",
    "d_rate      = 0.5\n",
    "reg_value   = 0.01\n",
    "\n",
    "# ------------------\n",
    "\n",
    "base_path = temp_directory\n",
    "\n",
    "model = build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# ----------------\n",
    "\n",
    "SAVE_CKPT_FREQ = 5\n",
    "steps_per_epoch = int(np.ceil(len(image_list_tr) / BATCH_SIZE))\n",
    "boundaries = [50 * steps_per_epoch]\n",
    "values = [l_rate, l_rate / 10]\n",
    "\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')\n",
    "\n",
    "checkpoint_directory = base_path / ('checkpoint_'+str(l_rate)+\"_\"+str(dim_k))\n",
    "\n",
    "if not os.path.exists(checkpoint_directory):\n",
    "    os.mkdir(checkpoint_directory)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_directory, max_to_keep=3)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "\n",
    "train_score = F1Score(num_classes=max_answers, average='micro', name='train_score')\n",
    "val_score = F1Score(num_classes=max_answers, average='micro', name='val_score')\n",
    "\n",
    "train_score = F1Score(num_classes=max_answers, average='micro', name='train_score')\n",
    "val_score = F1Score(num_classes=max_answers, average='micro', name='val_score')\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Functions\n",
    "# ------------------------\n",
    "\n",
    "# @tf.function\n",
    "def train_step(model, img, ques, ans, optimizer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model([img, ques], training=True)\n",
    "    loss = loss_object(ans, predictions)\n",
    "\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_score(ans, predictions)\n",
    "    \n",
    "  grads_ = list(zip(grads, model.trainable_variables))\n",
    "  return grads_\n",
    "\n",
    "def test_step(model, img, ques, ans):\n",
    "  predictions = model([img, ques])\n",
    "  loss = loss_object(ans, predictions)\n",
    "\n",
    "  val_loss(loss)\n",
    "  val_score(ans, predictions)\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "if manager.latest_checkpoint:\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    START_EPOCH = int(manager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ\n",
    "    print(\"Resume training from epoch: {}\".format(START_EPOCH))\n",
    "else:\n",
    "    print(\"Initializing from scratch\")\n",
    "    START_EPOCH = 0\n",
    "\n",
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "# -------------\n",
    "\n",
    "if TRAIN_ATTENTION:\n",
    "    for epoch in range(START_EPOCH, EPOCHS):\n",
    "\n",
    "      start = time.time()\n",
    "\n",
    "      for img, ques, ans in tqdm(dataset_tr):\n",
    "        grads = train_step(model, img, ques, ans, optimizer)\n",
    "\n",
    "      with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('f1_score', train_score.result(), step=epoch)\n",
    "        \n",
    "        for var in model.trainable_variables:\n",
    "            tf.summary.histogram(var.name, var, step=epoch)\n",
    "        for grad, var in grads:\n",
    "            tf.summary.histogram(var.name + '/gradient', grad, step=epoch)\n",
    "\n",
    "      for img, ques, ans in tqdm(dataset_vl):\n",
    "        test_step(model, img, ques, ans)\n",
    "\n",
    "      with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('f1_score', val_score.result(), step=epoch)\n",
    "        \n",
    "      template = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, val loss: {:.4f}, val f1_score: {:.4f}, time: {:.0f} sec'\n",
    "      print (template.format(epoch + 1,\n",
    "                             train_loss.result(), \n",
    "                             train_score.result(),\n",
    "                             val_loss.result(), \n",
    "                             val_score.result(),\n",
    "                             (time.time() - start)))\n",
    "\n",
    "      train_loss.reset_states()\n",
    "      train_score.reset_states()\n",
    "      val_loss.reset_states()\n",
    "      val_score.reset_states()\n",
    "      ckpt.step.assign_add(1)\n",
    "      if int(ckpt.step) % SAVE_CKPT_FREQ == 0:\n",
    "          manager.save()\n",
    "          print('Saved checkpoint.')\n",
    "\n",
    "# -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Auxiliary Functions\n",
    "# ------------------------------\n",
    "\n",
    "def predict(image_feature,processed_question):\n",
    "    answer = model([image_feature,processed_question])\n",
    "    answer = tf.argmax(answer, axis=1, output_type=tf.int32)\n",
    "    answer = (labelencoder.inverse_transform(answer))\n",
    "    return answer\n",
    "\n",
    "def preprocess_data(image_id,question):\n",
    "    features = np.load(str(ROOT_DIR / 'features' / (image_id+'.npy')))\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "\n",
    "    processed_question = process_sentence(question)\n",
    "    tok.fit_on_texts(processed_question)\n",
    "    processed_question = [processed_question]\n",
    "    processed_question = tok.texts_to_sequences(processed_question)\n",
    "    processed_question = sequence.pad_sequences(processed_question, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "    return features, processed_question\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')\n",
    "\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# ----------------\n",
    "\n",
    "test_annotations = json.load(open(ROOT_DIR / 'test_questions.json', 'r'))\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "results = {}\n",
    "\n",
    "if TEST_PREDICTION:\n",
    "    for i in tqdm(test_annotations):\n",
    "        value = test_annotations[i]\n",
    "        image_features, processed_question = preprocess_data(value['image_id'],value['question'])\n",
    "        answer = predict(image_features,processed_question)\n",
    "        results[i] = labels_dict.get(answer[0])\n",
    "    \n",
    "    create_csv(results, str(ROOT_DIR))\n",
    "\n",
    "# ----------------"
   ]
  }
 ]
}